{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d67a4729-cd2f-47e7-a4f6-f84a5677414f",
      "metadata": {
        "id": "d67a4729-cd2f-47e7-a4f6-f84a5677414f"
      },
      "source": [
        "# Basic RAG\n",
        "Retrieval-augmented generation (RAG) is an AI framework that synergizes the capabilities of LLMs and information retrieval systems. It’s useful to answer questions or generate content leveraging external knowledge. There are two main steps in RAG: 1) retrieval: retrieve relevant information from a knowledge base with text embeddings stored in a vector store; 2) generation: insert the relevant information to the prompt for the LLM to generate information. In this file, we will build a basic example of RAG with four implementations:\n",
        "\n",
        "- RAG from scratch with Mistral\n",
        "- RAG with Mistral and LangChain\n",
        "- RAG with Mistral and LlamaIndex\n",
        "\n",
        "### Import needed packages\n",
        "The first step is to install the needed packages `mistralai` and `faiss-cpu` and import the needed packages:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b880d1ed-3db0-45a1-807e-1b47e9ce1320",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b880d1ed-3db0-45a1-807e-1b47e9ce1320",
        "outputId": "6e39c69a-cc84-42a6-91cf-1e49306a1ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu==1.7.4\n",
            "  Using cached faiss-cpu-1.7.4.tar.gz (57 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: mistralai in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.4.0)\n",
            "Requirement already satisfied: httpx<0.26,>=0.25 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from mistralai) (0.25.2)\n",
            "Requirement already satisfied: orjson<3.11,>=3.9.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from mistralai) (3.10.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from mistralai) (2.7.3)\n",
            "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<0.26,>=0.25->mistralai) (4.4.0)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<0.26,>=0.25->mistralai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<0.26,>=0.25->mistralai) (1.0.5)\n",
            "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<0.26,>=0.25->mistralai) (3.7)\n",
            "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<0.26,>=0.25->mistralai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.26,>=0.25->mistralai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->mistralai) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->mistralai) (4.12.1)\n",
            "Building wheels for collected packages: faiss-cpu\n",
            "  Building wheel for faiss-cpu (pyproject.toml) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for faiss-cpu \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[8 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m running bdist_wheel\n",
            "  \u001b[31m   \u001b[0m running build\n",
            "  \u001b[31m   \u001b[0m running build_py\n",
            "  \u001b[31m   \u001b[0m running build_ext\n",
            "  \u001b[31m   \u001b[0m building 'faiss._swigfaiss' extension\n",
            "  \u001b[31m   \u001b[0m swigging faiss/faiss/python/swigfaiss.i to faiss/faiss/python/swigfaiss_wrap.cpp\n",
            "  \u001b[31m   \u001b[0m swig -python -c++ -Doverride= -I/usr/local/include -Ifaiss -doxygen -module swigfaiss -o faiss/faiss/python/swigfaiss_wrap.cpp faiss/faiss/python/swigfaiss.i\n",
            "  \u001b[31m   \u001b[0m error: command 'swig' failed: No such file or directory\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for faiss-cpu\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build faiss-cpu\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (faiss-cpu)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip3 install faiss-cpu==1.7.4 mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "851612c3-ee93-42e3-a1fb-481f89c9410f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "851612c3-ee93-42e3-a1fb-481f89c9410f",
        "outputId": "1827bdea-8532-4ab7-9d96-8ddda2de7057"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mistralai'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/lexi/Documents/Documents - Lexi’s MacBook Pro/RAG_KG/RAG_and_with_KG.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmistralai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m MistralClient\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmistralai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_completion\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatMessage\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mistralai'"
          ]
        }
      ],
      "source": [
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "import requests\n",
        "import numpy as np\n",
        "import faiss\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "api_key= os.getenv(\"mistral_api_key\")\n",
        "client = MistralClient(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe8609d5-9f27-4202-b0be-36db34412998",
      "metadata": {
        "id": "fe8609d5-9f27-4202-b0be-36db34412998"
      },
      "source": [
        "### Get data\n",
        "\n",
        "In this very simple example, we are getting data from the late payment policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c4c01740-72b4-482c-b61e-e272a734f01f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c4c01740-72b4-482c-b61e-e272a734f01f",
        "outputId": "4e92cfe8-eaea-4959-e18c-531eae366cd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "Late Payment Policy (FM1) \n",
            " \n",
            "1 \n",
            "July 2019 version \n",
            " \n",
            "The University of British Columbia  \n",
            "Board of Governors \n",
            " \n",
            " \n",
            "Policy No.: \n",
            "FM1 \n",
            "Long Title: \n",
            "Late Payment of Fees and Accounts \n",
            " \n",
            "Short Title:  \n",
            "Late Payment Policy \n",
            " \n",
            " \n",
            " \n",
            "1. General \n",
            " \n",
            "1.1 \n",
            "Where  fees,  fines,  or  other  indebtedness  to  the  University  remain  unpaid  despite  the \n",
            "University having taken reasonable steps to notify the individual concerned, the University \n",
            "may report the outstanding obligation to credit reporting agencies, commence legal action, \n",
            "or utilize any other remedies that may be available to it, whether the outstanding obligation \n",
            "is owed by a faculty member, staff member, student, or other individual. \n",
            " \n",
            "1.2 \n",
            "A late payment fee and interest may be charged. \n",
            " \n",
            "1.3 \n",
            "In cases where the outstanding obligation is owed by a student, the University will attempt to \n",
            "secure payment using internal processes prior to commencing any legal action. Provided that \n",
            "the  University  has  first  taken  reasonable  steps  to  notify  the  individual  concerned,  such \n",
            "internal  processes  may  include  refraining  from  making  additional  services  or  privileges \n",
            "available  to  the  student.  Without  limiting  the  generality  of  the  foregoing,  the  University, \n",
            "acting through Enrolment Services, may decline to: \n",
            " \n",
            "1.3.1 \n",
            "process an application for admission as a student; \n",
            " \n",
            "1.3.2 \n",
            "allow subsequent registration; or \n",
            " \n",
            "1.3.3 \n",
            "provide academic transcripts or otherwise make grade information available. \n",
            " \n",
            "1.4 \n",
            "Notwithstanding  anything  else  in  this  Policy,  individual  academic  departments  within  the \n",
            "University are not authorized to withhold grades from Enrolment Services for any reason. \n",
            " \n",
            "1.5 \n",
            "Where frees, fines, or other indebtedness to the University remain unpaid, the University may \n",
            "charge late fees or interest. \n",
            " \n",
            " \n",
            " \n",
            "Procedures to Late Payment Policy (FM1) \n",
            " \n",
            "1 \n",
            "July 2002 version \n",
            " \n",
            " PROCEDURES ASSOCIATED WITH THE  \n",
            "LATE PAYMENT POLICY \n",
            " \n",
            " \n",
            "Pursuant to the Regulatory Framework Policy, the President may approve Procedures or the amendment \n",
            "or  repeal  of  Procedures.  Such  approvals  must  be  reported  at  the  next  meeting  of  the  UBC  Board  of \n",
            "Governors or as soon thereafter as practicable. \n",
            " \n",
            "Capitalized terms used in these Procedures that are not otherwise defined herein shall have the meanings \n",
            "given to such terms in the accompanying Policy, being the Late Payment Policy. \n",
            " \n",
            "1. General \n",
            " \n",
            "1.1 \n",
            "Where  fees,  fines,  or  other  indebtedness  to  the  University  are  incurred  and  remain \n",
            "outstanding, the administrative unit in which the outstanding obligation was incurred shall \n",
            "take reasonable steps to notify the individual concerned before taking any further steps. Such \n",
            "notification shall state the late fees or interest charges, if any, which apply to the outstanding \n",
            "obligation, as well as the potential consequences of non‐payment. \n",
            " \n",
            "1.2 \n",
            "Where  the  outstanding  obligation  remains  unpaid  despite  the  foregoing  attempts  at \n",
            "notification, the administrative unit may decline to provide further services to the individual \n",
            "concerned. Without limiting the generality of the foregoing: \n",
            " \n",
            "1.2.1 \n",
            "the Department of Housing and Conferences may refuse admission to residences and \n",
            "may withdraw residence privileges, including dining privileges, requiring a resident to \n",
            "vacate the premises; \n",
            " \n",
            "1.2.2 \n",
            "Parking and Access Control Services may withdraw parking privileges and may tow \n",
            "vehicles; and \n",
            " \n",
            "1.2.3 \n",
            "the  Library  may  withdraw  borrowing  privileges  and  access  to  its  collection  of \n",
            "electronic information resources. \n",
            " \n",
            "1.3 \n",
            "Where the outstanding obligation was incurred by a student, the administrative unit may also \n",
            "forward all information relating to the outstanding obligation to Enrolment Services. Where \n",
            "an administrative unit chooses to do so, it must first have established protocols in cooperation \n",
            "with Enrolment Services so as to ensure that if the administrative unit receives payment of \n",
            "the  outstanding  obligation,  Enrolment  Services  is  notified  on  a  real‐time  basis.  Where \n",
            "outstanding obligations are referred to Enrolment Services, Enrolment Services may add an \n",
            "administrative fee to the outstanding obligation. \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Procedures to Late Payment Policy (FM1) \n",
            " \n",
            "2 \n",
            "July 2002 version \n",
            "1.4 \n",
            "If an administrative unit forwards information to Enrolment Services as contemplated by the \n",
            "preceding paragraph, the administrative unit must at the same time also take reasonable \n",
            "steps to notify the student that until the obligation is paid in full, Enrolment Services will not: \n",
            " \n",
            "1.4.1 \n",
            "process an application for admission as a student; \n",
            " \n",
            "1.4.2 \n",
            "allow subsequent registration; or \n",
            " \n",
            "1.4.3 \n",
            "provide academic transcripts or otherwise make grade information available. \n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_path='Late-Payment-Policy_FM1.txt'\n",
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text_content = file.read()\n",
        "    return text_content\n",
        "text = read_text_file(file_path)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "69c0f072",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "Study Leave Policy (HR8) \n",
            " \n",
            "1 \n",
            "July 2019 version \n",
            " \n",
            "The University of British Columbia  \n",
            "Board of Governors \n",
            " \n",
            " \n",
            "Policy No.: \n",
            "HR8 \n",
            "Long Title: \n",
            "Study Leave (Other Than Faculty) \n",
            " \n",
            "Short Title:  \n",
            "Study Leave Policy \n",
            " \n",
            " \n",
            " \n",
            "1. General \n",
            " \n",
            "1.1 \n",
            "Heads  and  Directors  of  Academic  Service  Departments  and  of  Administrative  Service \n",
            "Departments, and the Associate and Assistant Heads or Directors of these Departments, as well \n",
            "as other professional staff as approved from time to time by the President, are eligible to apply \n",
            "for a study leave program. The prescribed conditions are as follows: \n",
            " \n",
            "1.1.1 \n",
            "Eligibility ‐ After four years (4) continuous service. \n",
            " \n",
            "1.1.2 \n",
            "Entitlement ‐ Three (3) plus the number of years of full‐time service equals the number \n",
            "of months of partially paid study leave to a maximum of one year. \n",
            " \n",
            "2. Study Leave Salary \n",
            " \n",
            "2.1 \n",
            "Fifty  percent  (50%)  of  basic  salary  together  with  the  University’s  full  contribution  to  fringe \n",
            "benefits, provided the individual continues his/her own contributions. Where the basic salary \n",
            "and contributions are paid in whole or in part from non‐University funds (e.g., grants), the \n",
            "University can guarantee only that proportion of study leave salary and contributions to pension \n",
            "and other benefits which derive from University general revenues. It is the responsibility of the \n",
            "individual to determine whether the non‐University fund may be charged for the proportionate \n",
            "share of study leave salary, and contributions to pension and other benefits during the period \n",
            "of leave. If so, the individual shall present to the University certification attesting to this. Neither \n",
            "the University nor  the outside fund agency is obligated,  however, to  continue  the full fifty \n",
            "percent (50%) of salary if the individual’s total remuneration from salary, fellowships, grants \n",
            "(excluding research and travelling expenses) exceeds one‐hundred percent (100%) of full salary. \n",
            " \n",
            "3. Study Leave Program \n",
            " \n",
            "3.1 \n",
            "Study  leave  permits  the  member  of  staff  to  pursue  study  of  direct  benefit  to  the  job,  as \n",
            "approved by the University. \n",
            " \n",
            "3.1.1 \n",
            "It is assumed that, on expiration of leave, the individual will return to his/her duties at \n",
            "this University. \n",
            " \n",
            "Study Leave Policy (HR8) \n",
            " \n",
            "2 \n",
            "July 2019 version \n",
            "3.1.2 \n",
            "Study  leave  with  partial  salary  will  not  be  given  for  the  purpose  of  meeting  basic \n",
            "qualifications.  Leave for this purpose may be granted without salary, although in this \n",
            "case the University will make its normal contributions to his/her pension and fringe \n",
            "benefits, subject to the individual’s contributing his/her share. \n",
            " \n",
            "4. Authorization of Study Leave \n",
            " \n",
            "4.1 \n",
            "The application form for leave, with the Head or Director’s signature, should be submitted to \n",
            "the appropriate Vice‐President and/or Deputy Vice‐Chancellor as early as possible, preferably \n",
            "one year in advance. The Vice‐President and/or the Deputy Vice‐Chancellor will review the \n",
            "application and submit his recommendation to the President. \n",
            " \n",
            "5. Budgetary Arrangements \n",
            " \n",
            "5.1 \n",
            "The department from which the staff member is proceeding on approved study leave will not \n",
            "be provided with any supplementary funds for his/her replacement. The financial arrangements \n",
            "for a replacement will be limited to the amount of unused salary provided from University \n",
            "general  revenues.  If  the  salary  of  the  individual  is  provided  in  whole  or  in  part  from  non‐\n",
            "University funds (e.g. grants), then it is the responsibility of the individual to determine whether \n",
            "the unused amount of salary from these funds may be used for payment of a replacement \n",
            "during the period of leave. If so, then the individual must present to the University certification \n",
            "attesting to this. \n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_path2='Study-Leave-Policy_HR8.txt'\n",
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text_content = file.read()\n",
        "    return text_content\n",
        "text2 = read_text_file(file_path2)\n",
        "print(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "f03f47af-a20b-4122-a114-74b9748ff543",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f03f47af-a20b-4122-a114-74b9748ff543",
        "outputId": "224dcb72-3472-495f-eac1-5e98946f309d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3747"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text)\n",
        "len(text2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aad1aa61-9e1c-46c8-ae5e-61855df440f9",
      "metadata": {
        "id": "aad1aa61-9e1c-46c8-ae5e-61855df440f9"
      },
      "source": [
        "## Split document into chunks\n",
        "\n",
        "In a RAG system, it is crucial to split the document into smaller chunks so that it’s more effective to identify and retrieve the most relevant information in the retrieval process later. In this example, we simply split our text by character, combine 4765 characters into each chunk, and we get 10 chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "8494655e-bd87-49de-8f1d-69ffbc1c256e",
      "metadata": {
        "id": "8494655e-bd87-49de-8f1d-69ffbc1c256e"
      },
      "outputs": [],
      "source": [
        "chunk_size = 512\n",
        "chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "11ab9440",
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_size = 512\n",
        "chunks = [text2[i:i + chunk_size] for i in range(0, len(text2), chunk_size)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "c78c9936-0c1d-471c-b030-6c45639e7238",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c78c9936-0c1d-471c-b030-6c45639e7238",
        "outputId": "31e7f256-68e6-4c0c-9a24-8da88d36276e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4176cbe3-9b15-4d17-afb1-665011d09bb7",
      "metadata": {
        "id": "4176cbe3-9b15-4d17-afb1-665011d09bb7"
      },
      "source": [
        "#### Considerations:\n",
        "- **Chunk size**: Depending on your specific use case, it may be necessary to customize or experiment with different chunk sizes and chunk overlap to achieve optimal performance in RAG. For example, smaller chunks can be more beneficial in retrieval processes, as larger text chunks often contain filler text that can obscure the semantic representation. As such, using smaller text chunks in the retrieval process can enable the RAG system to identify and extract relevant information more effectively and accurately.  However, it’s worth considering the trade-offs that come with using smaller chunks, such as increasing processing time and computational resources.\n",
        "- **How to split**: While the simplest method is to split the text by character, there are other options depending on the use case and document structure. For example, to avoid exceeding token limits in API calls, it may be necessary to split the text by tokens. To maintain the cohesiveness of the chunks, it can be useful to split the text by sentences, paragraphs, or HTML headers. If working with code, it’s often recommended to split by meaningful code chunks for example using an Abstract Syntax Tree (AST) parser.\n",
        "\n",
        "\n",
        "### Create embeddings for each text chunk\n",
        "For each text chunk, we then need to create text embeddings, which are numeric representations of the text in the vector space. Words with similar meanings are expected to be in closer proximity or have a shorter distance in the vector space.\n",
        "To create an embedding, use Mistral’s embeddings API endpoint and the embedding model `mistral-embed`. We create a `get_text_embedding` to get the embedding from a single text chunk and then we use list comprehension to get text embeddings for all text chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "e77d9805-7a53-4210-9f80-f4de52285588",
      "metadata": {
        "id": "e77d9805-7a53-4210-9f80-f4de52285588"
      },
      "outputs": [],
      "source": [
        "def get_text_embedding(input):\n",
        "    embeddings_batch_response = client.embeddings(\n",
        "          model=\"mistral-embed\",\n",
        "          input=input\n",
        "      )\n",
        "    return embeddings_batch_response.data[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "46503830-6ad5-493e-a629-152721e2d88e",
      "metadata": {
        "id": "46503830-6ad5-493e-a629-152721e2d88e"
      },
      "outputs": [
        {
          "ename": "MistralAPIException",
          "evalue": "Status: 403. Message: {\"message\":\"Inactive subscription or usage limit reached\"}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMistralAPIException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/lexi/Documents/RAG_KG/RAG_and_with_KG.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lexi/Documents/RAG_KG/RAG_and_with_KG.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m text_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([get_text_embedding(chunk) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks])\n",
            "\u001b[1;32m/Users/lexi/Documents/RAG_KG/RAG_and_with_KG.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lexi/Documents/RAG_KG/RAG_and_with_KG.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_text_embedding\u001b[39m(\u001b[39minput\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lexi/Documents/RAG_KG/RAG_and_with_KG.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     embeddings_batch_response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lexi/Documents/RAG_KG/RAG_and_with_KG.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m           model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmistral-embed\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lexi/Documents/RAG_KG/RAG_and_with_KG.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m           \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lexi/Documents/RAG_KG/RAG_and_with_KG.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m       )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lexi/Documents/RAG_KG/RAG_and_with_KG.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings_batch_response\u001b[39m.\u001b[39mdata[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39membedding\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/mistralai/client.py:283\u001b[0m, in \u001b[0;36mMistralClient.embeddings\u001b[0;34m(self, model, input)\u001b[0m\n\u001b[1;32m    280\u001b[0m request \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model, \u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39minput\u001b[39m}\n\u001b[1;32m    281\u001b[0m singleton_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request(\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, request, \u001b[39m\"\u001b[39m\u001b[39mv1/embeddings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m \u001b[39mfor\u001b[39;49;00m response \u001b[39min\u001b[39;49;00m singleton_response:\n\u001b[1;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;49;00m EmbeddingResponse(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse)\n\u001b[1;32m    286\u001b[0m \u001b[39mraise\u001b[39;00m MistralException(\u001b[39m\"\u001b[39m\u001b[39mNo response received\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/mistralai/client.py:142\u001b[0m, in \u001b[0;36mMistralClient._request\u001b[0;34m(self, method, json, path, stream, attempt, data, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    134\u001b[0m             method,\n\u001b[1;32m    135\u001b[0m             url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    140\u001b[0m         )\n\u001b[0;32m--> 142\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_response(response)\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    145\u001b[0m     \u001b[39mraise\u001b[39;00m MistralConnectionException(\u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/mistralai/client.py:75\u001b[0m, in \u001b[0;36mMistralClient._check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_response\u001b[39m(\u001b[39mself\u001b[39m, response: Response) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_response_status_codes(response)\n\u001b[1;32m     77\u001b[0m     json_response: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m json_response:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/mistralai/client.py:60\u001b[0m, in \u001b[0;36mMistralClient._check_response_status_codes\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstream:\n\u001b[1;32m     59\u001b[0m         response\u001b[39m.\u001b[39mread()\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mraise\u001b[39;00m MistralAPIException\u001b[39m.\u001b[39mfrom_response(\n\u001b[1;32m     61\u001b[0m         response,\n\u001b[1;32m     62\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStatus: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m. Message: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m     )\n\u001b[1;32m     64\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m500\u001b[39m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstream:\n",
            "\u001b[0;31mMistralAPIException\u001b[0m: Status: 403. Message: {\"message\":\"Inactive subscription or usage limit reached\"}"
          ]
        }
      ],
      "source": [
        "text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca875993-fe6d-42df-811e-a43891cd0350",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca875993-fe6d-42df-811e-a43891cd0350",
        "outputId": "f4ea0eed-e595-431e-e034-dd0cea56f46e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 1024)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55396758-c3f3-45b3-b6e7-d4912c0899f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55396758-c3f3-45b3-b6e7-d4912c0899f2",
        "outputId": "3f93e48e-5280-4e7e-f720-ca153f6f8f6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.06896973,  0.03579712,  0.04455566, ..., -0.01472473,\n",
              "         0.00475311,  0.01698303],\n",
              "       [-0.05960083,  0.03814697,  0.02752686, ..., -0.002388  ,\n",
              "         0.01235199,  0.02053833],\n",
              "       [-0.06817627,  0.03305054,  0.03308105, ..., -0.00751495,\n",
              "        -0.01160431,  0.01058197],\n",
              "       ...,\n",
              "       [-0.07092285,  0.02958679,  0.04815674, ..., -0.01805115,\n",
              "         0.00206375,  0.01121521],\n",
              "       [-0.05944824,  0.02044678,  0.0317688 , ..., -0.00389099,\n",
              "         0.01121521, -0.00862885],\n",
              "       [-0.0475769 ,  0.02705383,  0.03640747, ..., -0.02383423,\n",
              "        -0.00164032, -0.00032401]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cba33c7-9d1d-44d8-a01e-e30f16be1aac",
      "metadata": {
        "id": "1cba33c7-9d1d-44d8-a01e-e30f16be1aac"
      },
      "source": [
        "### Load into a vector database\n",
        "Once we get the text embeddings, a common practice is to store them in a vector database for efficient processing and retrieval. There are several vector database to choose from. In our simple example, we are using an open-source vector database Faiss, which allows for efficient similarity search.  \n",
        "\n",
        "With Faiss, we instantiate an instance of the Index class, which defines the indexing structure of the vector database. We then add the text embeddings to this indexing structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a5b1877-b113-4527-9055-cae9049fef08",
      "metadata": {
        "id": "6a5b1877-b113-4527-9055-cae9049fef08"
      },
      "outputs": [],
      "source": [
        "d = text_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(text_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee023ab-b26c-4df5-8a7b-7dd660bfad86",
      "metadata": {
        "id": "5ee023ab-b26c-4df5-8a7b-7dd660bfad86"
      },
      "source": [
        "#### Considerations:\n",
        "- **Vector database**: When selecting a vector database, there are several factors to consider including speed, scalability, cloud management, advanced filtering, and open-source vs. closed-source.\n",
        "\n",
        "### Create embeddings for a question\n",
        "Whenever users ask a question, we also need to create embeddings for this question using the same embedding models as before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "894d9764-9da9-4629-8f2a-c9dcaf6ceb8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "894d9764-9da9-4629-8f2a-c9dcaf6ceb8d",
        "outputId": "56a00dc8-86f2-47b5-cbeb-6b9a4bb249ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 1024)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"What may the University charge when fees, fines, or other indebtedness remain unpaid?\"\n",
        "question_embeddings = np.array([get_text_embedding(question)])\n",
        "question_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c4948cc-6d8b-449f-bc00-abb3591c7222",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c4948cc-6d8b-449f-bc00-abb3591c7222",
        "outputId": "82b5c46e-dc95-42e3-fbc5-e4e5a735f97a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.02053833,  0.0625    ,  0.04031372, ..., -0.01794434,\n",
              "        -0.00689316, -0.00888824]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15989e10-d0ec-41be-b6be-fa317565a926",
      "metadata": {
        "id": "15989e10-d0ec-41be-b6be-fa317565a926"
      },
      "source": [
        "#### Considerations:\n",
        "- Hypothetical Document Embeddings (HyDE): In some cases, the user’s question might not be the most relevant query to use for identifying the relevant context. Instead, it maybe more effective to generate a hypothetical answer or a hypothetical document based on the user’s query and use the embeddings of the generated text to retrieve similar text chunks.\n",
        "\n",
        "### Retrieve similar chunks from the vector database\n",
        "We can perform a search on the vector database with `index.search`, which takes two arguments: the first is the vector of the question embeddings, and the second is the number of similar vectors to retrieve. This function returns the distances and the indices of the most similar vectors to the question vector in the vector database. Then based on the returned indices, we can retrieve the actual relevant text chunks that correspond to those indices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c930b378-7aac-434c-881b-ab69d3edb93d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c930b378-7aac-434c-881b-ab69d3edb93d",
        "outputId": "ee0e09a2-c744-47d1-92ee-6b84f304973c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 5]]\n"
          ]
        }
      ],
      "source": [
        "D, I = index.search(question_embeddings, k=2)\n",
        "print(I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73aab584-1dbf-4532-b41e-0403eeeeb567",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73aab584-1dbf-4532-b41e-0403eeeeb567",
        "outputId": "9a5cc344-d084-492b-b5b5-d06ddc03157e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\xa0legal\\xa0action,\\xa0\\nor\\xa0utilize\\xa0any\\xa0other\\xa0remedies\\xa0that\\xa0may\\xa0be\\xa0available\\xa0to\\xa0it,\\xa0whether\\xa0the\\xa0outstanding\\xa0obligation\\xa0\\nis\\xa0owed\\xa0by\\xa0a\\xa0faculty\\xa0member,\\xa0staff\\xa0member,\\xa0student,\\xa0or\\xa0other\\xa0individual.\\xa0\\n\\xa0\\n1.2 \\nA\\xa0late\\xa0payment\\xa0fee\\xa0and\\xa0interest\\xa0may\\xa0be\\xa0charged.\\xa0\\n\\xa0\\n1.3 \\nIn\\xa0cases\\xa0where\\xa0the\\xa0outstanding\\xa0obligation\\xa0is\\xa0owed\\xa0by\\xa0a\\xa0student,\\xa0the\\xa0University\\xa0will\\xa0attempt\\xa0to\\xa0\\nsecure\\xa0payment\\xa0using\\xa0internal\\xa0processes\\xa0prior\\xa0to\\xa0commencing\\xa0any\\xa0legal\\xa0action.\\xa0Provided\\xa0that\\xa0\\nthe\\xa0 University\\xa0 has\\xa0 first\\xa0 taken\\xa0 reasonable\\xa0 steps\\xa0 to\\xa0 notify\\xa0 the\\xa0 ind', '\\xa0unit\\xa0in\\xa0which\\xa0the\\xa0outstanding\\xa0obligation\\xa0was\\xa0incurred\\xa0shall\\xa0\\ntake\\xa0reasonable\\xa0steps\\xa0to\\xa0notify\\xa0the\\xa0individual\\xa0concerned\\xa0before\\xa0taking\\xa0any\\xa0further\\xa0steps.\\xa0Such\\xa0\\nnotification\\xa0shall\\xa0state\\xa0the\\xa0late\\xa0fees\\xa0or\\xa0interest\\xa0charges,\\xa0if\\xa0any,\\xa0which\\xa0apply\\xa0to\\xa0the\\xa0outstanding\\xa0\\nobligation,\\xa0as\\xa0well\\xa0as\\xa0the\\xa0potential\\xa0consequences\\xa0of\\xa0non‐payment.\\xa0\\n\\xa0\\n1.2 \\nWhere\\xa0 the\\xa0 outstanding\\xa0 obligation\\xa0 remains\\xa0 unpaid\\xa0 despite\\xa0 the\\xa0 foregoing\\xa0 attempts\\xa0 at\\xa0\\nnotification,\\xa0the\\xa0administrative\\xa0unit\\xa0may\\xa0decline\\xa0to\\xa0provide\\xa0further\\xa0services\\xa0to\\xa0the\\xa0in']\n"
          ]
        }
      ],
      "source": [
        "retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
        "print(retrieved_chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b417a59-021a-411d-a491-cb31815192cd",
      "metadata": {
        "id": "4b417a59-021a-411d-a491-cb31815192cd"
      },
      "source": [
        "#### Considerations:\n",
        "- **Retrieval methods**: There are a lot different retrieval strategies. In our example, we are showing a simple similarity search with embeddings. Sometimes when there is metadata available for the data, it’s better to filter the data based on the metadata first before performing similarity search. There are also other statistical retrieval methods like TF-IDF and BM25 that use frequency and distribution of terms in the document to identify relevant text chunks.\n",
        "- **Retrieved document**: Do we always retrieve individual text chunk as it is? Not always.\n",
        "    - Sometimes, we would like to include more context around the actual retrieved text chunk. We call the actual retrieve text chunk “child chunk” and our goal is to retrieve a larger “parent chunk” that the “child chunk” belongs to.\n",
        "    - On occasion, we might also want to provide weights to our retrieve documents. For example, a time-weighted approach would help us retrieve the most recent document.\n",
        "    - One common issue in the retrieval process is the “lost in the middle” problem where the information in the middle of a long context gets lost. Our models have tried to mitigate this issue. For example, in the passkey task, our models have demonstrated the ability to find a \"needle in a haystack\" by retrieving a randomly inserted passkey within a long prompt, up to 32k context length. However, it is worth considering experimenting with reordering the document to determine if placing the most relevant chunks at the beginning and end leads to improved results.\n",
        "  \n",
        "### Combine context and question in a prompt and generate response\n",
        "\n",
        "Finally, we can offer the retrieved text chunks as the context information within the prompt. Here is a prompt template where we can include both the retrieved text and user question in the prompt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da042a53-4564-4057-9a60-9b57dffff6a1",
      "metadata": {
        "id": "da042a53-4564-4057-9a60-9b57dffff6a1"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e77d975b-5f69-4e9c-8b94-97214517eac7",
      "metadata": {
        "id": "e77d975b-5f69-4e9c-8b94-97214517eac7"
      },
      "outputs": [],
      "source": [
        "def run_mistral(user_message, model=\"mistral-medium-latest\"):\n",
        "    messages = [\n",
        "        ChatMessage(role=\"user\", content=user_message)\n",
        "    ]\n",
        "    chat_response = client.chat(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c5c20aa-6673-4105-9c10-886a1e18da8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "1c5c20aa-6673-4105-9c10-886a1e18da8a",
        "outputId": "e7535ef4-2cab-459a-cd81-f27bd258ba43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'According to the context information, if fees, fines, or other indebtedness remain unpaid, the University may charge a late payment fee and interest on the outstanding obligation. Additionally, the University may attempt to secure payment using internal processes prior to commencing legal action. In cases where the obligation is owed by a student, the administrative unit may decline to provide further services to the individual if the outstanding obligation remains unpaid despite attempts at notification. It is important to note that the administrative unit in which the outstanding obligation was incurred shall take reasonable steps to notify the individual concerned before taking any further steps. Such notification shall state the late fees or interest charges, if any, which apply to the outstanding obligation, as well as the potential consequences of non-payment.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_mistral(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e3b531c-4730-4108-ae8a-8de6563e085b",
      "metadata": {
        "id": "4e3b531c-4730-4108-ae8a-8de6563e085b"
      },
      "source": [
        "#### Considerations:\n",
        "- Prompting techniques: Most of the prompting techniques can be used in developing a RAG system as well. For example, we can use few-shot learning to guide the model’s answers by providing a few examples. Additionally, we can explicitly instruct the model to format answers in a certain way.\n",
        "\n",
        "\n",
        "In the next sections, we are going to show you how to do a similar basic RAG with some of the popular RAG frameworks. We will start with LlamaIndex and add other frameworks in the future.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f538ee-c561-43a6-a421-a0e7b35638f7",
      "metadata": {
        "id": "48f538ee-c561-43a6-a421-a0e7b35638f7"
      },
      "source": [
        "## LlamaIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "152c2a1e-9564-459c-9ea9-5208da519a90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "152c2a1e-9564-459c-9ea9-5208da519a90",
        "outputId": "ec6f0e6f-598f-4ccb-da91-ec320b1af118",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.10.46)\n",
            "Requirement already satisfied: llama-index-llms-mistralai in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.1.16)\n",
            "Requirement already satisfied: llama-index-embeddings-mistralai in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.1.4)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.2.7)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.1.12)\n",
            "Requirement already satisfied: llama-index-core==0.10.46 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.10.46)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.1.10)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.1.22)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.1.25)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index) (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.46->llama-index) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (2024.6.0)\n",
            "Requirement already satisfied: httpx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (0.25.2)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/lexi/Library/Python/3.12/lib/python/site-packages (from llama-index-core==0.10.46->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (1.34.0)\n",
            "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (10.3.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity<8.4.0,>=8.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (4.12.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-core==0.10.46->llama-index) (1.16.0)\n",
            "Requirement already satisfied: mistralai>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-llms-mistralai) (0.4.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index) (0.4.4)\n",
            "Requirement already satisfied: orjson<3.11,>=3.9.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from mistralai>=0.4.0->llama-index-llms-mistralai) (3.10.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from mistralai>=0.4.0->llama-index-llms-mistralai) (2.7.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.46->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.46->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.46->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.46->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.46->llama-index) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx->llama-index-core==0.10.46->llama-index) (4.4.0)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx->llama-index-core==0.10.46->llama-index) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx->llama-index-core==0.10.46->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx->llama-index-core==0.10.46->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx->llama-index-core==0.10.46->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core==0.10.46->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.46->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.46->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.46->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core==0.10.46->llama-index) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->mistralai>=0.4.0->llama-index-llms-mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->mistralai>=0.4.0->llama-index-llms-mistralai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core==0.10.46->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core==0.10.46->llama-index) (2.2.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.46->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.46->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dataclasses-json->llama-index-core==0.10.46->llama-index) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->llama-index-core==0.10.46->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->llama-index-core==0.10.46->llama-index) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->llama-index-core==0.10.46->llama-index) (2024.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.46->llama-index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.46->llama-index) (1.16.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install llama-index llama-index-llms-mistralai llama-index-embeddings-mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96003762-acac-4886-964b-2d6a67f6f724",
      "metadata": {
        "id": "96003762-acac-4886-964b-2d6a67f6f724"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71df0de1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "reader = SimpleDirectoryReader(input_files=[\"Late-Payment-Policy_FM1.txt\"])\n",
        "documents = reader.load_data()\n",
        "# Define LLM and embedding model\n",
        "Settings.llm = MistralAI(model=\"open-mistral-7b\", api_key=api_key)\n",
        "Settings.embed_model = MistralAIEmbedding(model_name='mistral-embed', api_key=api_key)\n",
        "# Create vector store index\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "izhnf8UMzl_7",
      "metadata": {
        "id": "izhnf8UMzl_7"
      },
      "outputs": [],
      "source": [
        "# Create query engine\n",
        "query_engine = index.as_query_engine(similarity_top_k=2)\n",
        "response = query_engine.query(\n",
        "    \"What may the University charge when fees, fines, or other indebtedness remain unpaid?\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KRKAC19kzUK6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRKAC19kzUK6",
        "outputId": "2439a276-a4d5-4f14-ee1b-e000ddbd6e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The University may charge late fees or interest when fees, fines, or other\n",
            "indebtedness remain unpaid.\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "def format_response(response):\n",
        "    response_text = str(response)\n",
        "    return '\\n'.join(textwrap.wrap(response_text, width=80))  # Wrap text at 80 characters\n",
        "\n",
        "formatted_response = format_response(response)\n",
        "print(formatted_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m1Tk5qfyAhLJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1Tk5qfyAhLJ",
        "outputId": "0a29c9f2-d5ee-410b-d12a-bfbc698991b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No, individual academic departments are not authorized to withhold grades from\n",
            "Enrolment Services for any reason, according to the Late-Payment-Policy_FM1.txt.\n",
            "This is stated in section 1.4 of the July 2019 version of the policy.\n"
          ]
        }
      ],
      "source": [
        "response2 = query_engine.query(\n",
        "    \"Are individual academic departments authorized to withhold grades from Enrolment Services for any reason?\"\n",
        ")\n",
        "\n",
        "formatted_response2 = format_response(response2)\n",
        "print(formatted_response2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41oWtmRbBC7y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41oWtmRbBC7y",
        "outputId": "76955d03-f358-4c47-bb78-b08715aaf75a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An administrative unit must take reasonable steps to notify the individual\n",
            "concerned before declining further services. This notification should state the\n",
            "late fees or interest charges, if any, which apply to the outstanding\n",
            "obligation, as well as the potential consequences of non-payment.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "response3 = query_engine.query(\n",
        "    \"What steps must an administrative unit take before declining further services to an individual with outstanding obligations?\"\n",
        ")\n",
        "formatted_response3 = format_response(response3)\n",
        "print(formatted_response3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bpQOjEd2BgxZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpQOjEd2BgxZ",
        "outputId": "69108444-afc9-405b-8b36-9ac97bc9aca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Department of Housing and Conferences may refuse admission to residences and\n",
            "may withdraw residence privileges, including dining privileges, requiring a\n",
            "resident to vacate the premises.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "response4 = query_engine.query(\n",
        "    \"What actions may the Department of Housing and Conferences take if a resident has outstanding obligations?\")\n",
        "formatted_response4 = format_response(response4)\n",
        "print(formatted_response4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oz6g2sZnCCGi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz6g2sZnCCGi",
        "outputId": "718cab98-a433-4c66-94d9-e4446c2c2700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parking and Access Control Services may withdraw parking privileges and may tow\n",
            "vehicles.\n"
          ]
        }
      ],
      "source": [
        "response5 = query_engine.query(\n",
        "    \"What may Parking and Access Control Services do in cases of unpaid obligations?\")\n",
        "formatted_response5 = format_response(response5)\n",
        "print(formatted_response5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee69919",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "reader_sl = SimpleDirectoryReader(input_files=[\"Study-Leave-Policy_HR8.txt\"])\n",
        "documents = reader_sl.load_data()\n",
        "# Define LLM and embedding model\n",
        "Settings.llm = MistralAI(model=\"open-mistral-7b\", api_key=api_key)\n",
        "Settings.embed_model = MistralAIEmbedding(model_name='mistral-embed', api_key=api_key)\n",
        "# Create vector store index\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b09a4dab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create query engine\n",
        "query_engine = index.as_query_engine(similarity_top_k=2)\n",
        "response_sl_1 = query_engine.query(\n",
        "    \"What are the entitlements for study leave?\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d68103a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The entitlement for study leave, according to the provided context, is three\n",
            "months plus the number of years of full-time service, up to a maximum of one\n",
            "year. This means that after four years of continuous service, an individual is\n",
            "entitled to a study leave of up to one year, with three months being the base\n",
            "and additional months being equal to the number of years of full-time service.\n",
            "This study leave is partially paid, with the individual receiving 50% of their\n",
            "basic salary, along with the University's full contribution to fringe benefits,\n",
            "provided the individual continues their own contributions.\n"
          ]
        }
      ],
      "source": [
        "formatted_response_sl_1 = format_response(response_sl_1)\n",
        "print(formatted_response_sl_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87dc18a7",
      "metadata": {},
      "source": [
        "# Langchain + Mistral AI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ERWWquOi4EDb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERWWquOi4EDb",
        "outputId": "a0d11116-fd23-4793-fbdc-375e320e8532"
      },
      "outputs": [],
      "source": [
        "!pip3 install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VQSSM_dU4SHf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQSSM_dU4SHf",
        "outputId": "b01f41bb-de65-4be9-9a83-81ae2b263afe"
      },
      "outputs": [],
      "source": [
        "!pip3 install langchain_mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "LlbSupJQ3jnh",
      "metadata": {
        "id": "LlbSupJQ3jnh"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from langchain_mistralai.embeddings import MistralAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "import requests\n",
        "import numpy as np\n",
        "import faiss\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "api_key= os.getenv(\"mistral_api_key\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "frJquMzs4Pmh",
      "metadata": {
        "id": "frJquMzs4Pmh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "loader = TextLoader(\"Late-Payment-Policy_FM1.txt\")\n",
        "docs = loader.load()\n",
        "# Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)\n",
        "# Define the embedding model\n",
        "embeddings = MistralAIEmbeddings(model=\"mistral-embed\", mistral_api_key=api_key)\n",
        "# Create the vector store\n",
        "vector = FAISS.from_documents(documents, embeddings)\n",
        "# Define a retriever interface\n",
        "retriever = vector.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "HdVNNFqY30Ly",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdVNNFqY30Ly",
        "outputId": "5883bdaf-f995-46c6-a22b-31c98078223d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When fees, fines, or other indebtedness to the University remain unpaid, the University may charge late fees or interest, as stated in the Late Payment Policy (FM1) under section 1.5. The specific amount of late fees or interest that may be charged is not specified in the context provided.\n"
          ]
        }
      ],
      "source": [
        "# Define LLM\n",
        "model = ChatMistralAI(mistral_api_key=api_key)\n",
        "# Define prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "# Create a retrieval chain to answer questions\n",
        "document_chain = create_stuff_documents_chain(model, prompt)\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "response = retrieval_chain.invoke({\"input\": \"What may the University charge when fees, fines, or other indebtedness remain unpaid\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "CNM5EydLBxif",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNM5EydLBxif",
        "outputId": "c15e9851-eacf-45b3-bbc7-a748499fc5ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Department of Housing and Conferences may refuse admission to residences and may withdraw residence privileges, including dining privileges, requiring a resident to vacate the premises if the resident has outstanding obligations.\n"
          ]
        }
      ],
      "source": [
        "response2 = retrieval_chain.invoke({\"input\": \"What actions may the Department of Housing and Conferences take if a resident has outstanding obligations?\"})\n",
        "print(response2[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "SXSi_3g4CbTl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXSi_3g4CbTl",
        "outputId": "f7ea9f6a-9f72-41cc-8bcf-9bb779a64fc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parking and Access Control Services may withdraw parking privileges and may tow vehicles in cases of unpaid obligations.\n"
          ]
        }
      ],
      "source": [
        "response2 = retrieval_chain.invoke({\"input\": \"What may Parking and Access Control Services do in cases of unpaid obligations?\"})\n",
        "print(response2[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b41147d3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No, individual academic departments are not authorized to withhold grades from Enrolment Services for any reason. This is stated in section 1.4 of the Late Payment Policy (FM1) from July 2019 version.\n"
          ]
        }
      ],
      "source": [
        "response3 = retrieval_chain.invoke({\"input\": \"Are individual academic departments authorized to withhold grades from Enrolment Services for any reason?\"})\n",
        "print(response3[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "75e5e2c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If fees, fines, or other indebtedness to the University remain unpaid despite the University's attempts to notify the individual concerned, the University may take several actions. These can include reporting the outstanding obligation to credit reporting agencies, commencing legal action, or utilizing any other remedies available. The University may also charge a late payment fee and interest.\n",
            "\n",
            "In cases where the outstanding obligation is owed by a student, the University will attempt to secure payment using internal processes prior to commencing any legal action. These internal processes may include refraining from making additional services or privileges available to the student. Specifically, the University, acting through Enrolment Services, may decline to process an application for admission as a student, allow subsequent registration, or provide academic transcripts or otherwise make grade information available.\n",
            "\n",
            "However, individual academic departments within the University are not authorized to withhold grades from Enrolment Services for any reason. Where fees, fines, or other indebtedness to the University remain unpaid, the University may charge late fees or interest.\n"
          ]
        }
      ],
      "source": [
        "response4 = retrieval_chain.invoke({\"input\": \"What actions may the University take if fees, fines, or other indebtedness remain unpaid?\"})\n",
        "print(response4[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c8ee4bad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided context, the internal processes the University may use to secure payment from a student before commencing legal action include:\n",
            "\n",
            "1.3.1 Declining to process an application for admission as a student.\n",
            "1.3.2 Declining to allow subsequent registration.\n",
            "1.3.3 Declining to provide academic transcripts or otherwise make grade information available.\n",
            "\n",
            "These steps are taken through Enrolment Services, which manages the student's academic record and related services. The University will attempt to use these internal processes prior to commencing any legal action, provided that it has first taken reasonable steps to notify the individual concerned.\n"
          ]
        }
      ],
      "source": [
        "response5 = retrieval_chain.invoke({\"input\": \"What are the internal processes the University may use to secure payment from a student before commencing legal action?\"})\n",
        "print(response5[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4654421c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "According to the provided context, before an administrative unit at the University of British Columbia declines further services to an individual with outstanding obligations, they must take the following steps:\n",
            "\n",
            "1. Take reasonable steps to notify the individual concerned about the outstanding obligation, stating the late fees or interest charges, if any, as well as the potential consequences of non-payment.\n",
            "2. If the outstanding obligation remains unpaid despite the attempts at notification, the administrative unit may decline to provide further services to the individual. However, this should not be done without first notifying the individual.\n",
            "\n",
            "It's important to note that the specific consequences of non-payment may vary depending on the administrative unit. For example, the Department of Housing and Conferences may refuse admission to residences and withdraw residence privileges, Parking and Access Control Services may withdraw parking privileges and tow vehicles, and the Library may withdraw borrowing privileges and access to its collection of electronic information resources.\n",
            "\n",
            "Additionally, if the outstanding obligation was incurred by a student, the administrative unit may also forward all information relating to the outstanding obligation to Enrolment Services. However, this must only be done if the administrative unit has first established protocols in cooperation with Enrolment Services to ensure that if they receive payment, Enrolment Services is notified on a real-time basis.\n"
          ]
        }
      ],
      "source": [
        "response6 = retrieval_chain.invoke({\"input\": \"What steps must an administrative unit take before declining further services to an individual with outstanding obligations?\"})\n",
        "print(response6[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6ca2fcaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "According to the provided context, if an individual has outstanding obligations to the University of British Columbia, the Library may withdraw borrowing privileges and access to its collection of electronic information resources. This is outlined in section 1.2.3 of the procedures associated with the Late Payment Policy.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "response7 = retrieval_chain.invoke({\"input\": \"What may the Library do if an individual has outstanding obligations?\"})\n",
        "print(response7[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "367be61e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If an administrative unit forwards information about a student’s outstanding obligation to Enrolment Services, the administrative unit must at the same time take reasonable steps to notify the student that until the obligation is paid in full, Enrolment Services will not process an application for admission as a student, allow subsequent registration, or provide academic transcripts or otherwise make grade information available (as per section 1.4 of the July 2002 version of the Procedures to Late Payment Policy (FM1)). Additionally, Enrolment Services may add an administrative fee to the outstanding obligation (as per the context provided).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "response8 = retrieval_chain.invoke({\"input\": \"What steps must be taken if an administrative unit forwards information about a student’s outstanding obligation to Enrolment Services?\"})\n",
        "print(response8[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "06ea2462",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, based on the context provided, it is stated that \"Where outstanding obligations are referred to Enrolment Services, Enrolment Services may add an administrative fee to the outstanding obligation.\" This implies that Enrolment Services has the authority to add an administrative fee to an outstanding obligation that has been referred to them by an administrative unit.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "response9 = retrieval_chain.invoke({\"input\": \"Can Enrolment Services add an administrative fee to an outstanding obligation referred to them by an administrative unit?\"})\n",
        "print(response9[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7682bf1",
      "metadata": {},
      "source": [
        "# Integrate KG with RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2173ce97",
      "metadata": {},
      "source": [
        "### Connecting neo4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "151bd256",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "\n",
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9409385",
      "metadata": {},
      "source": [
        "# Late Payment KG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d461096d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "load_dotenv('.env', override=True)\n",
        "NEO4J_URI = os.getenv('NEO4J_URI')\n",
        "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
        "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
        "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c84bc410",
      "metadata": {},
      "outputs": [],
      "source": [
        "kg = Neo4jGraph(\n",
        "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "0b255487",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node properties:\n",
            "Organization {name: STRING}\n",
            "Role {name: STRING}\n",
            "Rule {description: STRING, name: STRING}\n",
            "Consequence {name: STRING}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "(:Organization)-[:HAS]->(:Organization)\n",
            "(:Rule)-[:MAY_LEAD_TO]->(:Consequence)\n",
            "(:Rule)-[:APPLIES_TO]->(:Consequence)\n",
            "(:Rule)-[:APPLIES_TO]->(:Role)\n",
            "(:Rule)-[:AFFECTS]->(:Role)\n",
            "(:Rule)-[:AFFECTS]->(:Organization)\n",
            "(:Rule)-[:AFFECTS]->(:Consequence)\n",
            "(:Rule)-[:MAY_RESULT_IN]->(:Consequence)\n",
            "(:Rule)-[:RESTRICTS]->(:Consequence)\n"
          ]
        }
      ],
      "source": [
        "print(kg.schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ced2aad5",
      "metadata": {},
      "outputs": [],
      "source": [
        "cypher = \"\"\"\n",
        "  MATCH (n) \n",
        "  RETURN count(n)\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "736f10d5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'count(n)': 46}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = kg.query(cypher)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d45f5ad3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "def query_neo4j(query, parameters=None):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query, parameters)\n",
        "        return result.data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "25da3bea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_openai in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.1.9)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain_openai) (0.2.9)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain_openai) (1.34.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (0.1.75)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (0.25.2)\n",
            "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "6c6279bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import textwrap\n",
        "\n",
        "# Langchain\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from neo4j import GraphDatabase\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "79ae9c7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node properties: Organization {name: STRING} Role {name:\n",
            "STRING} Rule {description: STRING, name: STRING} Consequence\n",
            "{name: STRING} Relationship properties:  The relationships:\n",
            "(:Organization)-[:HAS]->(:Organization)\n",
            "(:Rule)-[:MAY_LEAD_TO]->(:Consequence)\n",
            "(:Rule)-[:APPLIES_TO]->(:Consequence)\n",
            "(:Rule)-[:APPLIES_TO]->(:Role) (:Rule)-[:AFFECTS]->(:Role)\n",
            "(:Rule)-[:AFFECTS]->(:Organization)\n",
            "(:Rule)-[:AFFECTS]->(:Consequence)\n",
            "(:Rule)-[:MAY_RESULT_IN]->(:Consequence)\n",
            "(:Rule)-[:RESTRICTS]->(:Consequence)\n"
          ]
        }
      ],
      "source": [
        "kg.refresh_schema()\n",
        "print(textwrap.fill(kg.schema, 60))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d975ae74",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kg.query(\"\"\"\n",
        "    MATCH (rule:Rule {name: 'Late Payment Policy'})-[:MAY_RESULT_IN]->(consequence:Consequence)\n",
        "    RETURN rule.description, consequence.name\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e08f221",
      "metadata": {},
      "source": [
        "# Study Leave KG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba13ec30",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv('.env', override=True)\n",
        "NEO4J_URI_SL = os.getenv('NEO4J_URI')\n",
        "NEO4J_USERNAME_SL = os.getenv('NEO4J_USERNAME')\n",
        "NEO4J_PASSWORD_SL = os.getenv('NEO4J_PASSWORD_SL')\n",
        "NEO4J_DATABASE_SL = os.getenv('NEO4J_DATABASE')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a8e8ca8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node properties:\n",
            "StudyLeave {budgetaryArrangements: STRING, eligibility: STRING, entitlement: STRING, programDescription: STRING}\n",
            "StaffMember {title: STRING, department: STRING}\n",
            "Approver {responsibility: STRING, prerequisite: STRING, title: STRING}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "(:StaffMember)-[:APPLY_FOR]->(:StudyLeave)\n",
            "(:Approver)-[:APPROVE]->(:StudyLeave)\n",
            "(:Approver)-[:REVIEW]->(:StudyLeave)\n"
          ]
        }
      ],
      "source": [
        "graph_sl = Neo4jGraph(\n",
        "    url=NEO4J_URI_SL, username=NEO4J_USERNAME_SL, password=NEO4J_PASSWORD_SL, database=NEO4J_DATABASE_SL\n",
        ")\n",
        "graph_sl.refresh_schema()\n",
        "print(graph_sl.schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292ef463",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the template for generating Cypher queries based on the schema and user questions\n",
        "CYPHER_GENERATION_TEMPLATE = \"\"\"Task: Generate Cypher statement to \n",
        "query a graph database.\n",
        "Instructions:\n",
        "Use only the provided relationship types and properties in the \n",
        "schema. Do not use any other relationship types or properties that \n",
        "are not provided.\n",
        "Schema:\n",
        "{schema}\n",
        "Note: Do not include any explanations or apologies in your responses.\n",
        "Do not respond to any questions that might ask anything else than \n",
        "for you to construct a Cypher statement.\n",
        "Do not include any text except the generated Cypher statement.\n",
        "# Who are the staff members eligible for study leave?\n",
        "MATCH (sm:StaffMember)-[:APPLY_FOR]->(sl:StudyLeave)\n",
        "WHERE toLower(sl.eligibility) CONTAINS 'eligible'\n",
        "RETURN sm.title, sm.department\n",
        "\n",
        "# Which approver is responsible for approving study leave?\n",
        "MATCH (ap:Approver)-[:APPROVE]->(:StudyLeave)\n",
        "WHERE toLower(ap.responsibility) CONTAINS 'approve'\n",
        "RETURN ap.title, ap.responsibility\n",
        "\n",
        "# What responsibilities does the Vice-President have regarding study leave?\n",
        "MATCH (ap:Approver)\n",
        "WHERE toLower(ap.title) CONTAINS 'vice-president'\n",
        "RETURN ap.responsibility\n",
        "\n",
        "# What are the entitlements for study leave?\n",
        "MATCH (sl:StudyLeave)\n",
        "RETURN sl.entitlement\n",
        "\n",
        "The question is:\n",
        "{question}\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb18679e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA chain initialized\n"
          ]
        }
      ],
      "source": [
        "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n",
        ")\n",
        "\n",
        "# Initialize the GraphCypherQAChain\n",
        "# Initialize the GraphCypherQAChain without formatting the template here\n",
        "qa_chain = GraphCypherQAChain.from_llm(\n",
        "    ChatOpenAI(temperature=0),\n",
        "    graph=graph_sl,\n",
        "    verbose=True,\n",
        "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
        "    )\n",
        "print(\"QA chain initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d88fb96c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def prettyCypherChain(question: str) -> str:\n",
        "    response = qa_chain.run(question)\n",
        "    print(textwrap.fill(response, 60))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f404ec3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (sl:StudyLeave)\n",
            "RETURN sl.entitlement\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'sl.entitlement': 'Up to 12 months of partially paid leave, depending on the length of service'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Up to 12 months of partially paid leave, depending on the\n",
            "length of service.\n"
          ]
        }
      ],
      "source": [
        "prettyCypherChain(\"What are the entitlements for study leave?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a0477ee",
      "metadata": {},
      "source": [
        "# Vectored KG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d043c694",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv('.env', override=True)\n",
        "NEO4J_URI_VK = os.getenv('NEO4J_URI')\n",
        "NEO4J_USERNAME_VK = os.getenv('NEO4J_USERNAME')\n",
        "NEO4J_PASSWORD_VK = os.getenv('NEO4J_PASSWORD_VK')\n",
        "NEO4J_DATABASE_VK = os.getenv('NEO4J_DATABASE')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c206a79b",
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_VK = Neo4jGraph(\n",
        "    url=NEO4J_URI_VK, username=NEO4J_USERNAME_VK, password=NEO4J_PASSWORD_VK, database=NEO4J_DATABASE_VK\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09638d8",
      "metadata": {},
      "source": [
        "The graph retriever starts by identifying relevant entities in the input. For simplicity, we instruct the LLM to identify people, organizations, and actions. To achieve this, we will use LCEL with the newly added with_structured_output method to achieve this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "f56eebe4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retriever\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Tuple, List, Optional\n",
        "llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-0125\")\n",
        "kg.query(\n",
        "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
        "\n",
        "# Extract entities from text\n",
        "class Entities(BaseModel):\n",
        "    \"\"\"Identifying information about entities.\"\"\"\n",
        "\n",
        "    names: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"All the department, students, faculty, staff, actions, or business entities that \"\n",
        "        \"appear in the text\",\n",
        "    )\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are extracting organization and person entities from the text.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Use the given format to extract information from the following \"\n",
        "            \"input: {question}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "entity_chain = prompt | llm.with_structured_output(Entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "74f5799e",
      "metadata": {},
      "outputs": [
        {
          "ename": "InternalServerError",
          "evalue": "Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_53bd475c0d556cba449d4b57ab40e907 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/lexi/Documents/Documents - Lexi’s MacBook Pro/RAG_KG/RAG_and_with_KG.ipynb Cell 84\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y136sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m entity_chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mAre individual academic departments authorized to withhold grades from Enrolment Services for any reason?\u001b[39;49m\u001b[39m\"\u001b[39;49m})\u001b[39m.\u001b[39mnames\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:2504\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2502\u001b[0m             \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39minvoke(\u001b[39minput\u001b[39m, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   2503\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2504\u001b[0m             \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\u001b[39minput\u001b[39;49m, config)\n\u001b[1;32m   2505\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   2506\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:4573\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4567\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m   4568\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4569\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[1;32m   4570\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   4571\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4572\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Output:\n\u001b[0;32m-> 4573\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbound\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   4574\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   4575\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_merge_configs(config),\n\u001b[1;32m   4576\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs},\n\u001b[1;32m   4577\u001b[0m     )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    166\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m    167\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[1;32m    168\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[1;32m    169\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    171\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    172\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    173\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    174\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    175\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    176\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    177\u001b[0m             run_id\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mrun_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    178\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    179\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m    180\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    598\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 599\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    455\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[0;32m--> 456\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    457\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)  \u001b[39m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    444\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 446\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    447\u001b[0m                 m,\n\u001b[1;32m    448\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    449\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    450\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    451\u001b[0m             )\n\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    672\u001b[0m             messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    673\u001b[0m         )\n\u001b[1;32m    674\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:543\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    542\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 543\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(messages\u001b[39m=\u001b[39;49mmessage_dicts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    544\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:606\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    574\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    605\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    607\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    608\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    609\u001b[0m             {\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    616\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    617\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    618\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    619\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    620\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    621\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    622\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    623\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    624\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    625\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    626\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    627\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    628\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    629\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    630\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    631\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    632\u001b[0m             },\n\u001b[1;32m    633\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    634\u001b[0m         ),\n\u001b[1;32m    635\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    636\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    637\u001b[0m         ),\n\u001b[1;32m    638\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    639\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    640\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    641\u001b[0m     )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    922\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    923\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    924\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    925\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    926\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    927\u001b[0m     )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1004\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1006\u001b[0m         options,\n\u001b[1;32m   1007\u001b[0m         cast_to,\n\u001b[1;32m   1008\u001b[0m         retries,\n\u001b[1;32m   1009\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1010\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1011\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m   1004\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m   1006\u001b[0m         options,\n\u001b[1;32m   1007\u001b[0m         cast_to,\n\u001b[1;32m   1008\u001b[0m         retries,\n\u001b[1;32m   1009\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m   1010\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1011\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
            "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_53bd475c0d556cba449d4b57ab40e907 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}"
          ]
        }
      ],
      "source": [
        "entity_chain.invoke({\"question\": \"Are individual academic departments authorized to withhold grades from Enrolment Services for any reason?\"}).names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ad69473e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
        "def generate_full_text_query(input: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a full-text search query for a given input string.\n",
        "\n",
        "    This function constructs a query string suitable for a full-text search.\n",
        "    It processes the input string by splitting it into words and appending a\n",
        "    similarity threshold (~2 changed characters) to each word, then combines\n",
        "    them using the AND operator. Useful for mapping entities from user questions\n",
        "    to database values, and allows for some misspelings.\n",
        "    \"\"\"\n",
        "    full_text_query = \"\"\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    for word in words[:-1]:\n",
        "        full_text_query += f\" {word}~2 AND\"\n",
        "    full_text_query += f\" {words[-1]}~2\"\n",
        "    return full_text_query.strip()\n",
        "\n",
        "# Fulltext index query\n",
        "def structured_retriever(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Collects the neighborhood of entities mentioned\n",
        "    in the question\n",
        "    \"\"\"\n",
        "    result = \"\"\n",
        "    entities = entity_chain.invoke({\"question\": question})\n",
        "    for entity in entities.names:\n",
        "        response = graph_VK.query(\n",
        "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
        "            YIELD node,score\n",
        "            CALL {\n",
        "              WITH node\n",
        "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
        "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
        "              UNION ALL\n",
        "              WITH node\n",
        "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
        "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
        "            }\n",
        "            RETURN output LIMIT 50\n",
        "            \"\"\",\n",
        "            {\"query\": generate_full_text_query(entity)},\n",
        "        )\n",
        "        result += \"\\n\".join([el['output'] for el in response])\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "f1dda586",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'graph_VK' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/lexi/Documents/Documents - Lexi’s MacBook Pro/RAG_KG/RAG_and_with_KG.ipynb Cell 86\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(structured_retriever(\u001b[39m\"\u001b[39;49m\u001b[39mAre individual academic departments authorized to withhold grades from Enrolment Services for any reason?\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
            "\u001b[1;32m/Users/lexi/Documents/Documents - Lexi’s MacBook Pro/RAG_KG/RAG_and_with_KG.ipynb Cell 86\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m entities \u001b[39m=\u001b[39m entity_chain\u001b[39m.\u001b[39minvoke({\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m: question})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m entity \u001b[39min\u001b[39;00m entities\u001b[39m.\u001b[39mnames:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     response \u001b[39m=\u001b[39m graph_VK\u001b[39m.\u001b[39mquery(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m        \u001b[39m\u001b[39m\"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m        YIELD node,score\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m        CALL {\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m          WITH node\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m          MATCH (node)-[r:!MENTIONS]->(neighbor)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m          RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m          UNION ALL\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m          WITH node\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m          MATCH (node)<-[r:!MENTIONS]-(neighbor)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m          RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m        }\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m        RETURN output LIMIT 50\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m: generate_full_text_query(entity)},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([el[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m response])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lexi/Documents/Documents%20-%20Lexi%E2%80%99s%20MacBook%20Pro/RAG_KG/RAG_and_with_KG.ipynb#Y141sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "\u001b[0;31mNameError\u001b[0m: name 'graph_VK' is not defined"
          ]
        }
      ],
      "source": [
        "print(structured_retriever(\"Are individual academic departments authorized to withhold grades from Enrolment Services for any reason?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "7d1f6226",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.create.setVectorProperty' has been replaced by 'db.create.setNodeVectorProperty')} {position: line: 1, column: 72, offset: 71} for query: \"UNWIND $data AS row MATCH (n:`node_label`) WHERE elementId(n) = row.id CALL db.create.setVectorProperty(n, 'embedding', row.embedding) YIELD node RETURN count(*)\"\n"
          ]
        }
      ],
      "source": [
        "\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    OpenAIEmbeddings(),\n",
        "    url=NEO4J_URI_VK,\n",
        "    username=NEO4J_USERNAME_VK,\n",
        "    password=NEO4J_PASSWORD_VK,\n",
        "    search_type=\"hybrid\",\n",
        "    node_label=\"node_label\",\n",
        "    text_node_properties=[\"name\"],\n",
        "    embedding_node_property=\"embedding\"\n",
        ")\n",
        "def retriever(question: str):\n",
        "    print(f\"Search query: {question}\")\n",
        "    structured_data = structured_retriever(question)\n",
        "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
        "    final_data = f\"\"\"Structured data:\n",
        "{structured_data}\n",
        "Unstructured data:\n",
        "{\"#Document \". join(unstructured_data)}\n",
        "    \"\"\"\n",
        "    return final_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "906b32a1",
      "metadata": {},
      "source": [
        "## Defining the RAG chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "542a2874",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import (\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        "    ConfigurableField, \n",
        ")# Condense a chat history and follow-up question into a standalone question\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
        "in its original language.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"  # noqa: E501\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
        "\n",
        "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
        "    buffer = []\n",
        "    for human, ai in chat_history:\n",
        "        buffer.append(HumanMessage(content=human))\n",
        "        buffer.append(AIMessage(content=ai))\n",
        "    return buffer\n",
        "\n",
        "_search_query = RunnableBranch(\n",
        "    # If input includes chat_history, we condense it with the follow-up question\n",
        "    (\n",
        "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
        "            run_name=\"HasChatHistoryCheck\"\n",
        "        ),  # Condense follow-up question and chat into a standalone_question\n",
        "        RunnablePassthrough.assign(\n",
        "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
        "        )\n",
        "        | CONDENSE_QUESTION_PROMPT\n",
        "        | ChatOpenAI(temperature=0)\n",
        "        | StrOutputParser(),\n",
        "    ),\n",
        "    # Else, we have no chat history, so just pass through the question\n",
        "    RunnableLambda(lambda x : x[\"question\"]),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "7e800e8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Use natural language and be concise.\n",
        "Answer:\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"context\": _search_query | retriever,\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "1f508fe7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "e395d59b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search query: Are individual academic departments authorized to withhold grades from Enrolment Services for any reason?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: MENTIONS)} {position: line: 5, column: 32, offset: 166} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: MENTIONS)} {position: line: 9, column: 33, offset: 348} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: MENTIONS)} {position: line: 5, column: 32, offset: 166} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: MENTIONS)} {position: line: 9, column: 33, offset: 348} for query: \"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\\n            YIELD node,score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:!MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:!MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'No, individual academic departments are not authorized to withhold grades from Enrolment Services for any reason.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"Are individual academic departments authorized to withhold grades from Enrolment Services for any reason?\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "formats": "ipynb,py:light"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
